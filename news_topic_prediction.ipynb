{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ma8pgdBh1v_-",
        "WS10XJdYBvna",
        "O3bq9GOuE7l_",
        "UznaGHNcYli1"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Предсказание новостных тематик\n",
        "Задача предсказания тем новостей, в рамках открытого буткемпа \"First Step in NLP: 2.0\" от ФКН ВШЭ.\n",
        "\n",
        "Тематики новостей:\n",
        "* `Общество/Россия`: 0\n",
        "* `Экономика`: 1\n",
        "* `Силовые структуры`: 2\n",
        "* `Бывший СССР`: 3\n",
        "* `Спорт`: 4\n",
        "* `Забота о себе`: 5\n",
        "* `Строительство`: 6\n",
        "* `Туризм/Путешествия`: 7\n",
        "* `Наука и техника`: 8"
      ],
      "metadata": {
        "id": "Z_2_3etrbTap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подключим Google-диск"
      ],
      "metadata": {
        "id": "_IPrFzT-crXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "i3JHLR7XcpUv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aff19136-60c3-48e5-aacf-79ded2c16027"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установим и импортируем необходимые библиотеки"
      ],
      "metadata": {
        "id": "gu1uQ8TRcL9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install selenium\n",
        "!pip install pymorphy2"
      ],
      "metadata": {
        "id": "7QP97y3vcMf-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests as rq\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import pymorphy2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from IPython import display\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0J8KfNp8fUq",
        "outputId": "84a903e3-5cd8-44c1-ba18-03a390948545"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определим константы"
      ],
      "metadata": {
        "id": "mtlyvzv2caSH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "e_LV73X3bHQD"
      },
      "outputs": [],
      "source": [
        "SLEEP = 1 # пауза между чтением url, в секундах\n",
        "DEPTH = 10000 # количество ссылок на новости в каждой рубрике\n",
        "PAGES = 500 # количество новостей в рубрике для парсинга\n",
        "\n",
        "# путь к данным\n",
        "DATA_DIR = '/'\n",
        "SCRAPING_DIR = 'scraping/'\n",
        "LINKS_SUBDIR = \"links/\"\n",
        "NEWS_SUBDIR = \"news/\"\n",
        "DATASET_DIR = 'dataset/'\n",
        "TEST_DIR = 'test/'\n",
        "SUBMIT_DIR = 'submit/'\n",
        "\n",
        "YEAR_FROM = 2019\n",
        "YEAR_TILL = 2023\n",
        "MIN_TEXT_LEN = 10 # минимальная длина текстовых данных\n",
        "\n",
        "RANDOM_STATE = 42 # параметр рандомизации"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опишем класс для парсинга новости"
      ],
      "metadata": {
        "id": "5jyqiB8Ocw5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Article:\n",
        "    url: str = None\n",
        "    title: str = None\n",
        "    subtitle: str = None\n",
        "    content: str = None\n",
        "    datetime: str = None\n",
        "    category: int = 0"
      ],
      "metadata": {
        "id": "KpgMXIq2b9e6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем web-скрейпер"
      ],
      "metadata": {
        "id": "AnpZ-bYTc1FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
        "chrome_options.add_argument(\"headless\")\n",
        "chrome_options.add_argument(\"no-sandbox\")\n",
        "chrome_options.add_argument(\"disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ],
      "metadata": {
        "id": "mBBZ0qwnc1uQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Функции скрейпинга для источников новостей"
      ],
      "metadata": {
        "id": "sjTxw0IT1hU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### РИА Новости"
      ],
      "metadata": {
        "id": "ma8pgdBh1v_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция скачивания списка ссылок на новости выбранной тематики"
      ],
      "metadata": {
        "id": "ryIOp89u2Neg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ria_get_list(scraper_id, base_url, idx, topic):\n",
        "\n",
        "    links = []\n",
        "\n",
        "    news_count = 20 # кол-во новостей на странице\n",
        "    pages_count = DEPTH // news_count # кол-во итераций чтения страниц со ссылками\n",
        "\n",
        "    url = base_url + topic # формируем ссылку на страницу рубрики\n",
        "    print(url)\n",
        "    driver.get(url) # открываем страницу\n",
        "    time.sleep(SLEEP) # делаем паузу\n",
        "\n",
        "    # кликаем по кнопке \"загрузить еще\"\n",
        "    driver.execute_script(\n",
        "        \"document.getElementsByClassName('list-more')[0].click()\"\n",
        "    )\n",
        "    time.sleep(1) # делаем паузу\n",
        "\n",
        "    # скроллим страницу для автоматической подгрузки нужного количества новостей\n",
        "    for i in tqdm(range(pages_count), leave=False):\n",
        "        try:\n",
        "            driver.execute_script(\n",
        "                \"window.scrollTo(0, document.body.scrollHeight - 1200)\"\n",
        "            )\n",
        "            time.sleep(1) # делаем паузу\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # читаем ссылки на страницы новостей\n",
        "    elems = driver.find_elements(By.XPATH, \"//div[@class='list-item']//a[@class='list-item__title color-font-hover-only']\")\n",
        "    links = links + [elem.get_attribute('href') for elem in elems]\n",
        "\n",
        "    return links"
      ],
      "metadata": {
        "id": "iO7BmsJ4g9bZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция парсинга страниц новостей"
      ],
      "metadata": {
        "id": "Zzu6pIU6iGKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ria_parse_page(url):\n",
        "\n",
        "    # Create article data class object\n",
        "    article = Article()\n",
        "\n",
        "    # article url\n",
        "    article.url = url\n",
        "\n",
        "    # article id\n",
        "    s = re.findall(r\"\\d+.html\", article.url)[0]\n",
        "    article_id = s[: s.find(\".\")]\n",
        "\n",
        "    # load page\n",
        "    driver.get(article.url)\n",
        "    time.sleep(SLEEP)\n",
        "    html = driver.page_source\n",
        "\n",
        "    # article source\n",
        "    source = article.url[8 : article.url.find(\".\")]\n",
        "\n",
        "    # article object\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    obj = soup.find(\n",
        "        \"div\",\n",
        "        {\n",
        "            \"class\": lambda x: x and (x.find(f\"article m-article m-{source}\") > -1),\n",
        "            \"data-article-id\": article_id,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    if not obj:\n",
        "        obj = soup.find(\n",
        "            \"div\",\n",
        "            {\n",
        "                \"class\": lambda x: x and (x.find(f\"article m-video m-{source}\") > -1),\n",
        "                \"data-article-id\": article.id,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    # process article title\n",
        "    title = obj.find(\"div\", {\"class\": \"article__title\"})\n",
        "    title_2 = obj.find(\"h1\", {\"class\": \"article__title\"})\n",
        "\n",
        "    if title:\n",
        "        article.title = title.text.strip()\n",
        "    else:\n",
        "        article.title = title_2.text.strip() if title_2 else \"\"\n",
        "\n",
        "    # article subtitle\n",
        "    subtitle = obj.find(\"h1\", {\"class\": \"article__second-title\"})\n",
        "    article.subtitle = subtitle.text.strip() if subtitle else \"\"\n",
        "\n",
        "    # article content\n",
        "    article.content = obj.find(\n",
        "        \"div\", {\"class\": \"article__body js-mediator-article mia-analytics\"}\n",
        "    ).text.strip()\n",
        "\n",
        "    # article datetime\n",
        "    article.datetime = obj.find(\"div\", {\"class\": \"article__info-date\"}).find(\"a\").text\n",
        "\n",
        "    return article"
      ],
      "metadata": {
        "id": "sN2X8wqe2YkB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Фонтанка"
      ],
      "metadata": {
        "id": "WS10XJdYBvna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fontanka_get_list(scraper_id, base_url, idx, topic):\n",
        "\n",
        "    links = []\n",
        "\n",
        "    for year in range(YEAR_FROM, YEAR_TILL + 1):\n",
        "\n",
        "        try:\n",
        "            url = base_url + topic + f\"arc/{year}/all.html\" # формируем ссылку на страницу рубрики\n",
        "            print(url)\n",
        "            driver.get(url) # открываем общую страницу рубрики\n",
        "            time.sleep(SLEEP) # делаем паузу\n",
        "\n",
        "            # читаем дни недели\n",
        "            days = driver.find_elements(\n",
        "                By.XPATH,\n",
        "                '//*[@id=\"app\"]/div/div[4]/div[2]/div/section/div/div/div[2]/div[2]//a'\n",
        "            )\n",
        "            days_list = [d.get_attribute('href') for d in days]\n",
        "            for day_url in tqdm(days_list, leave=False):\n",
        "                try:\n",
        "                    driver.get(day_url)\n",
        "                    time.sleep(1) # делаем паузу\n",
        "\n",
        "                    # читаем ссылки на страницы новостей\n",
        "                    elems = driver.find_elements(\n",
        "                        By.XPATH,\n",
        "                        '//*[@id=\"app\"]/div/div[4]/div[2]/div/section/div/div/div[2]/div/ul/li/div[2]/div[1]/a[1]'\n",
        "                    )\n",
        "                    links = links + [elem.get_attribute('href') for elem in elems]\n",
        "                    r = re.compile(\"https://www.fontanka.ru/\\d{4}/\\d{2}/\\d{2}/\\d+/$\")\n",
        "                    links = list(filter(r.match, links))\n",
        "                    if len(links) >= DEPTH:\n",
        "                        break\n",
        "                except:\n",
        "                    pass\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    return links"
      ],
      "metadata": {
        "id": "usFWCEqDB9NH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fontanka_parse_page(url):\n",
        "\n",
        "    # Create article data class object\n",
        "    article = Article()\n",
        "\n",
        "    # article url\n",
        "    article.url = url\n",
        "\n",
        "    # load page\n",
        "    driver.get(article.url)\n",
        "    time.sleep(SLEEP)\n",
        "    html = driver.page_source\n",
        "\n",
        "    # article object\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    obj = soup.find(\"article\")\n",
        "\n",
        "    # process article title\n",
        "    title = obj.find(\"h1\")\n",
        "    article.title = title.text.strip()\n",
        "\n",
        "    # article subtitle\n",
        "    article.subtitle = \"\"\n",
        "\n",
        "    # article content\n",
        "    article.content = obj.find(\n",
        "        \"section\", {\"itemprop\": \"articleBody\"}\n",
        "    ).text.strip()\n",
        "\n",
        "    # article datetime\n",
        "    article.datetime = obj.find(\n",
        "        \"span\", {\"itemprop\": \"datePublished\"}\n",
        "    ).text\n",
        "\n",
        "    return article"
      ],
      "metadata": {
        "id": "w-LVSANOC5OW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Коммерсант"
      ],
      "metadata": {
        "id": "O3bq9GOuE7l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kommersant_get_list(scraper_id, base_url, idx, topic):\n",
        "\n",
        "    links = []\n",
        "\n",
        "    url = base_url + topic + '/month' # формируем ссылку на страницу рубрики\n",
        "    print(url)\n",
        "    driver.get(url) # открываем страницу\n",
        "\n",
        "    while True:\n",
        "      # кликаем по кнопке \"загрузить еще\"\n",
        "      driver.execute_script(\n",
        "          \"document.getElementsByClassName('ui-button ui-button--standart ui-nav ui-nav--prev')[0].click()\"\n",
        "      )\n",
        "      time.sleep(1) # делаем паузу\n",
        "      # читаем ссылки на страницы новостей\n",
        "      elems = driver.find_elements(By.XPATH, \"//h2[@class='uho__name rubric_lenta__item_name']/a[@class='uho__link uho__link--overlay']\")\n",
        "      links = links + [elem.get_attribute('href') for elem in elems]\n",
        "      if len(links) >= DEPTH:\n",
        "          break\n",
        "\n",
        "    time.sleep(SLEEP) # делаем паузу\n",
        "\n",
        "    return links"
      ],
      "metadata": {
        "id": "B-d52N6AE_6O"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def kommersant_parse_page(url):\n",
        "\n",
        "    # Create article data class object\n",
        "    article = Article()\n",
        "\n",
        "    # article url\n",
        "    article.url = url\n",
        "\n",
        "    # load page\n",
        "    driver.get(article.url)\n",
        "    time.sleep(SLEEP)\n",
        "    html = driver.page_source\n",
        "\n",
        "    # article object\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    obj = soup.find(\"div\", {\"class\": \"doc__body\"})\n",
        "\n",
        "    # process article title\n",
        "    title = obj.find(\"h1\")\n",
        "    article.title = title.text.strip()\n",
        "\n",
        "    # article subtitle\n",
        "    article.subtitle = \" \"\n",
        "\n",
        "    # article content\n",
        "    article.content = obj.find(\n",
        "        \"div\", {\"class\": \"article_text_wrapper js-search-mark\"}\n",
        "    ).text.strip()\n",
        "\n",
        "    # article datetime\n",
        "    article.datetime = driver.find_element(\n",
        "        By.XPATH, \"//article/div[1]/time\"\n",
        "    ).text\n",
        "\n",
        "    return article"
      ],
      "metadata": {
        "id": "MWmgvB64FTRg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Лента"
      ],
      "metadata": {
        "id": "UznaGHNcYli1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class lentaRu_parser:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _get_url(self, param_dict: dict) -> str:\n",
        "        \"\"\"\n",
        "        Возвращает URL для запроса json таблицы со статьями\n",
        "\n",
        "        url = 'https://lenta.ru/search/v2/process?'\\\n",
        "        + 'from=0&'\\                       # Смещение\n",
        "        + 'size=1000&'\\                    # Кол-во статей\n",
        "        + 'sort=2&'\\                       # Сортировка по дате (2), по релевантности (1)\n",
        "        + 'title_only=0&'\\                 # Точная фраза в заголовке\n",
        "        + 'domain=1&'\\                     # ??\n",
        "        + 'modified%2Cformat=yyyy-MM-dd&'\\ # Формат даты\n",
        "        + 'type=1&'\\                       # Материалы. Все материалы (0). Новость (1)\n",
        "        + 'bloc=4&'\\                       # Рубрика. Экономика (4). Все рубрики (0)\n",
        "        + 'modified%2Cfrom=2020-01-01&'\\\n",
        "        + 'modified%2Cto=2020-11-01&'\\\n",
        "        + 'query='                         # Поисковой запрос\n",
        "        \"\"\"\n",
        "        hasType = int(param_dict['type']) != 0\n",
        "        hasBloc = int(param_dict['bloc']) != 0\n",
        "\n",
        "        url = 'https://lenta.ru/search/v2/process?'\\\n",
        "        + 'from={}&'.format(param_dict['from'])\\\n",
        "        + 'size={}&'.format(param_dict['size'])\\\n",
        "        + 'sort={}&'.format(param_dict['sort'])\\\n",
        "        + 'title_only={}&'.format(param_dict['title_only'])\\\n",
        "        + 'domain={}&'.format(param_dict['domain'])\\\n",
        "        + 'modified%2Cformat=yyyy-MM-dd&'\\\n",
        "        + 'type={}&'.format(param_dict['type']) * hasType\\\n",
        "        + 'bloc={}&'.format(param_dict['bloc']) * hasBloc\\\n",
        "        + 'modified%2Cfrom={}&'.format(param_dict['dateFrom'])\\\n",
        "        + 'modified%2Cto={}&'.format(param_dict['dateTo'])\\\n",
        "        + 'query={}'.format(param_dict['query'])\n",
        "\n",
        "        return url\n",
        "\n",
        "\n",
        "    def _get_search_table(self, param_dict: dict) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Возвращает pd.DataFrame со списком статей\n",
        "        \"\"\"\n",
        "        url = self._get_url(param_dict)\n",
        "        r = rq.get(url)\n",
        "        search_table = pd.DataFrame(r.json()['matches'])\n",
        "\n",
        "        return search_table\n",
        "\n",
        "\n",
        "    def get_articles(self,\n",
        "                     param_dict,\n",
        "                     time_step = 7) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Функция для скачивания статей интервалами через каждые time_step дней\n",
        "\n",
        "        param_dict: dict\n",
        "        ### Параметры запроса\n",
        "        ###### project - раздел поиска, например, rbcnews\n",
        "        ###### category - категория поиска, например, TopRbcRu_economics\n",
        "        ###### dateFrom - с даты\n",
        "        ###### dateTo - по дату\n",
        "        ###### offset - смещение поисковой выдачи\n",
        "        ###### limit - лимит статей, максимум 100\n",
        "        ###### query - поисковой запрос (ключевое слово), например, РБК\n",
        "\n",
        "        \"\"\"\n",
        "        param_copy = param_dict.copy()\n",
        "        time_step = timedelta(days=time_step)\n",
        "        dateFrom = datetime.strptime(param_copy['dateFrom'], '%Y-%m-%d')\n",
        "        dateTo = datetime.strptime(param_copy['dateTo'], '%Y-%m-%d')\n",
        "        if dateFrom > dateTo:\n",
        "            raise ValueError('dateFrom should be less than dateTo')\n",
        "\n",
        "        out = pd.DataFrame()\n",
        "\n",
        "        while dateFrom <= dateTo:\n",
        "            param_copy['dateTo'] = (dateFrom + time_step).strftime('%Y-%m-%d')\n",
        "            if dateFrom + time_step > dateTo:\n",
        "                param_copy['dateTo'] = dateTo.strftime('%Y-%m-%d')\n",
        "            out = out.append(self._get_search_table(param_copy), ignore_index=True)\n",
        "            dateFrom += time_step + timedelta(days=1)\n",
        "            param_copy['dateFrom'] = dateFrom.strftime('%Y-%m-%d')\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "FB0_4D0XYqsV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lenta_get_list(scraper_id, base_url, idx, topic):\n",
        "    query = ''\n",
        "    offset = 0\n",
        "    size = 1000\n",
        "    sort = \"3\"\n",
        "    title_only = \"0\"\n",
        "    domain = \"1\"\n",
        "    material = \"0\"\n",
        "    time_step = 7\n",
        "\n",
        "    for year in range(YEAR_FROM, YEAR_TILL + 1):\n",
        "        dateFrom = str(year) + '-01-01'\n",
        "        dateTo = str(year) + '-01-31' # '-12-31'\n",
        "        param_dict = {\n",
        "                    'query'     : query,\n",
        "                    'from'      : str(offset),\n",
        "                    'size'      : str(size),\n",
        "                    'dateFrom'  : dateFrom,\n",
        "                    'dateTo'    : dateTo,\n",
        "                    'sort'      : sort,\n",
        "                    'title_only': title_only,\n",
        "                    'type'      : material,\n",
        "                    'bloc'      : topic,\n",
        "                    'domain'    : domain\n",
        "        }\n",
        "\n",
        "        parser = lentaRu_parser()\n",
        "\n",
        "        tbl = parser.get_articles(param_dict=param_dict, time_step=time_step)\n",
        "        print('topic:', topic)\n",
        "\n",
        "        tbl = tbl[['url', 'pubdate', 'title', 'text']]\n",
        "        tbl = tbl.rename(columns={'pubdate': 'datetime', 'text': 'content'})\n",
        "        tbl['subtitle'] = \" \"\n",
        "        tbl['category'] = idx\n",
        "\n",
        "        scraper_dir = DATA_DIR + SCRAPING_DIR + f\"{scraper_id}/\"\n",
        "\n",
        "        if not os.path.exists(scraper_dir + NEWS_SUBDIR):\n",
        "            os.mkdir(scraper_dir + NEWS_SUBDIR)\n",
        "\n",
        "        news_file_path = scraper_dir + NEWS_SUBDIR + f\"news_{idx}.csv\"\n",
        "        tbl.to_csv(news_file_path, sep=';', encoding='utf-8')\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "vNb86IPdLvkX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lenta_parse_page(url):\n",
        "    pass"
      ],
      "metadata": {
        "id": "IsQ0ooClL-6X"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Парсинг новостей"
      ],
      "metadata": {
        "id": "Wx9lENOm2nwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Список источников новостей"
      ],
      "metadata": {
        "id": "9PhCQ-udZ02D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scrapers = [\n",
        "    [\n",
        "        \"ria\", # id скрейпера\n",
        "        \"https://ria.ru/\", # базовый url\n",
        "        [\n",
        "            \"society/\",# ссылки на тематические разделы\n",
        "            \"economy/\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"tag_thematic_category_Stroitelstvo/\",\n",
        "            \"tag_thematic_category_Turizm/\",\n",
        "            \"science/\"\n",
        "        ],\n",
        "        ria_get_list, # функция скачивания списка элементов выбранной тематики\n",
        "        ria_parse_page # функция парсинга скачанных страниц\n",
        "    ],\n",
        "    [\n",
        "        \"fontanka\",\n",
        "        \"https://www.fontanka.ru/\",\n",
        "        [\n",
        "            \"society/\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"sport/\",\n",
        "            \"\",\n",
        "            \"stroy/\",\n",
        "            \"turizm/\",\n",
        "            \"\"\n",
        "        ],\n",
        "        fontanka_get_list,\n",
        "        fontanka_parse_page\n",
        "    ],\n",
        "    [\n",
        "        \"kommersant\",\n",
        "        \"https://www.kommersant.ru/archive/\",\n",
        "        [\n",
        "            \"rubric/7\",\n",
        "            \"rubric/3\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"rubric/9\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"\",\n",
        "            \"online/296\"\n",
        "        ],\n",
        "        kommersant_get_list,\n",
        "        kommersant_parse_page\n",
        "    ],\n",
        "    [\n",
        "        \"lenta\",\n",
        "        \"https://lenta.ru/\",\n",
        "        [\n",
        "            \"1\",\n",
        "            \"4\",\n",
        "            \"37\",\n",
        "            \"3\",\n",
        "            \"8\",\n",
        "            \"87\",\n",
        "            \"\",\n",
        "            \"48\",\n",
        "            \"5\"\n",
        "        ],\n",
        "        lenta_get_list,\n",
        "        lenta_parse_page\n",
        "    ]\n",
        "]"
      ],
      "metadata": {
        "id": "jcF8EhaAZ0VI"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сформируем списки ссылок на новости для каждой рубрики каждого из источников"
      ],
      "metadata": {
        "id": "1w-SFuT32tEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Scraping...')\n",
        "print('')\n",
        "\n",
        "for scraper in scrapers:\n",
        "    scraper_id, base_url, topics, get_list, _ = scraper\n",
        "\n",
        "    print(scraper_id)\n",
        "\n",
        "    scraper_dir = DATA_DIR + SCRAPING_DIR + f\"{scraper_id}/\"\n",
        "    if not os.path.exists(scraper_dir):\n",
        "        os.mkdir(scraper_dir)\n",
        "    if not os.path.exists(scraper_dir + LINKS_SUBDIR):\n",
        "        os.mkdir(scraper_dir + LINKS_SUBDIR)\n",
        "\n",
        "    for idx, topic in enumerate(topics):\n",
        "        if topic != \"\":\n",
        "            links = get_list(scraper_id, base_url, idx, topic)\n",
        "            if len(links):\n",
        "                links_file_path = scraper_dir + LINKS_SUBDIR + f\"links_{idx}.csv\"\n",
        "                df_links = pd.DataFrame(data=links, columns=['link'])\n",
        "                df_links.drop_duplicates(subset=['link'], inplace=True, ignore_index=True)\n",
        "                df_links.to_csv(links_file_path, sep=';', encoding='utf-8')\n",
        "\n",
        "    print('')\n",
        "\n",
        "print('Scraping done.')"
      ],
      "metadata": {
        "id": "GFMl94d180ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачаем новости для каждой рубрики"
      ],
      "metadata": {
        "id": "KN7hmxTEK4WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Parsing...')\n",
        "print('')\n",
        "\n",
        "for scraper in scrapers:\n",
        "    scraper_id, base_url, topics, _, parse_page = scraper\n",
        "\n",
        "    print(scraper_id)\n",
        "\n",
        "    scraper_dir = DATA_DIR + SCRAPING_DIR + f\"{scraper_id}/\"\n",
        "\n",
        "    if not os.path.exists(scraper_dir + NEWS_SUBDIR):\n",
        "        os.mkdir(scraper_dir + NEWS_SUBDIR)\n",
        "\n",
        "    for idx, topic in enumerate(topics):\n",
        "        if topic != \"\":\n",
        "            links_file_path = scraper_dir + LINKS_SUBDIR + f\"links_{idx}.csv\"\n",
        "            if os.path.exists(links_file_path):\n",
        "                df_links = pd.read_csv(links_file_path, delimiter=';', usecols=['link'])\n",
        "                links = df_links['link']\n",
        "\n",
        "                news_file_path = scraper_dir + NEWS_SUBDIR + f\"news_{idx}.csv\"\n",
        "                if os.path.exists(news_file_path):\n",
        "                    df_cur_news = pd.read_csv(news_file_path, delimiter=';')\n",
        "                    df_new_links = df_links.loc[~links.isin(df_cur_news['url'])]\n",
        "                    df_new_links.reset_index(inplace=True)\n",
        "                    new_links = df_new_links['link']\n",
        "                    news_file_exists = True\n",
        "                else:\n",
        "                    new_links = links\n",
        "                    news_file_exists = False\n",
        "                if len(new_links) > 0:\n",
        "                    print('topic ', idx, ':', len(new_links), 'new links')\n",
        "\n",
        "                    data = []\n",
        "                    for link in tqdm(new_links[:PAGES], leave=False):\n",
        "                        try:\n",
        "                            res = parse_page(link)\n",
        "                            res.category = idx\n",
        "                            data.append(res)\n",
        "                        except:\n",
        "                            pass\n",
        "                    df = pd.DataFrame(data=data)\n",
        "\n",
        "                    if news_file_exists:\n",
        "                        df = pd.concat([df_cur_news, df], axis=0, ignore_index=True)\n",
        "                        df.drop_duplicates(subset=['url'], inplace=True, ignore_index=True)\n",
        "\n",
        "                    df.to_csv(news_file_path, sep=';', encoding='utf-8')\n",
        "    print('')\n",
        "\n",
        "print('Parsing done.')"
      ],
      "metadata": {
        "id": "W-qVNR9i84H6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого источника объединим полученные новости в общий файл"
      ],
      "metadata": {
        "id": "YjfI-rwwDNqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Concatination...')\n",
        "print('')\n",
        "\n",
        "for scraper in scrapers:\n",
        "    scraper_id, base_url, topics, _, _ = scraper\n",
        "\n",
        "    scraper_dir = DATA_DIR + SCRAPING_DIR + f\"{scraper_id}/\"\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    for idx, topic in enumerate(topics):\n",
        "        if topic != \"\":\n",
        "            news_file_path = scraper_dir + NEWS_SUBDIR + f\"news_{idx}.csv\"\n",
        "            df = pd.read_csv(\n",
        "                news_file_path,\n",
        "                delimiter=';',\n",
        "                usecols=['url', 'title', 'subtitle', 'content', 'category'])\n",
        "            df_list.append(df)\n",
        "\n",
        "    df_news = pd.concat(df_list, axis=0, ignore_index=True)\n",
        "    df_news = df_news.fillna('')\n",
        "\n",
        "    print(scraper_id, ':', len(df_news), 'news')\n",
        "\n",
        "    df_news.to_csv(scraper_dir + \"news.csv\", sep=';', encoding='utf-8')\n",
        "\n",
        "print('')\n",
        "print('Concatination done.')"
      ],
      "metadata": {
        "id": "GHUA0rcm874Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединим все файлы новостей в один датасет"
      ],
      "metadata": {
        "id": "yPHU-pXDDknx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = []\n",
        "\n",
        "for scraper in scrapers:\n",
        "    scraper_id, base_url, topics, _, _ = scraper\n",
        "\n",
        "    scraper_dir = DATA_DIR + SCRAPING_DIR + f\"{scraper_id}/\"\n",
        "    df_news = pd.read_csv(scraper_dir + \"news.csv\", delimiter=';', usecols=['url', 'title', 'subtitle', 'content', 'category'])\n",
        "    df_list.append(df_news)\n",
        "\n",
        "df_news_all = pd.concat(df_list, axis=0, ignore_index=True)\n",
        "df_news_all.drop_duplicates(subset=['url'], inplace=True, ignore_index=True)"
      ],
      "metadata": {
        "id": "Y4PCyMv8DqFb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединим текстовые поля датафрейма"
      ],
      "metadata": {
        "id": "LtzFKBkRDVcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_news_all['content'] = df_news_all['title'] + '. ' + df_news_all['subtitle'] + '. ' + df_news_all['content']\n",
        "df_news_all.content = df_news_all.content.astype(str)\n",
        "df_news_all['content'] = [re.sub('[\\.\\?!]+(\\.) ', '\\\\1 ', t) for t in df_news_all['content']]\n",
        "df_news_all = df_news_all.drop(columns=['title', 'subtitle'])"
      ],
      "metadata": {
        "id": "Irw8-gqlDTRK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news_all.sample(5)"
      ],
      "metadata": {
        "id": "TFaEI9DY8_UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраним результат в файл"
      ],
      "metadata": {
        "id": "34hH1WFiD4Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_news_path = DATA_DIR + SCRAPING_DIR + \"news.csv\"\n",
        "df_news_all.drop('url', axis=1).to_csv(all_news_path, sep=';', encoding='utf-8')\n",
        "\n",
        "print('News file:', all_news_path)\n",
        "print('Total news:', len(df_news_all))"
      ],
      "metadata": {
        "id": "YNc0op7o9BX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка текстовых данных"
      ],
      "metadata": {
        "id": "r6LC75t-e-N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прочитаем CSV-файл и загрузим данные в датафрейм"
      ],
      "metadata": {
        "id": "wrlV2opZfMwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_file = DATA_DIR + SCRAPING_DIR + \"news.csv\"\n",
        "df = pd.read_csv(news_file, usecols=['content', 'category'], delimiter=';')"
      ],
      "metadata": {
        "id": "CPLv3zuKfCvs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим данные на наличие пропусков"
      ],
      "metadata": {
        "id": "yF6jsNPGfvHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "A2HSE2RV9DuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем распределение записей по категориям"
      ],
      "metadata": {
        "id": "iqz-ciTOfzLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['category'].value_counts()"
      ],
      "metadata": {
        "id": "Yr3ucPg39FuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['category'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "THpSueUy9JYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(15, 5)})\n",
        "sns.countplot(x='category', data=df)\n",
        "plt.xlabel(\"category\", size = 12)\n",
        "plt.ylabel(\"count\", size = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UbmtdVp89NO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функции подготовки данных"
      ],
      "metadata": {
        "id": "BIZ6d4U4hGKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаление пустых значений"
      ],
      "metadata": {
        "id": "YUDp1eNVhKFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_empty(df):\n",
        "    df.dropna(inplace=True)\n",
        "    return df"
      ],
      "metadata": {
        "id": "zm9LqhMuhOpF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строковый тип колонок `title` и `content`, категориальный тип колонки `category`"
      ],
      "metadata": {
        "id": "FUS4ddfrhRR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_category_type(df):\n",
        "    df = df.astype({'content': str, 'category': 'category'})\n",
        "    return df"
      ],
      "metadata": {
        "id": "pvjPzgvXhTbq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация текста (процесс разбиения текстового документа на отдельные фразы)"
      ],
      "metadata": {
        "id": "MlK-FFrxhWme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    sents = sent_tokenize(text, language='russian')\n",
        "    return sents"
      ],
      "metadata": {
        "id": "gmaZEZ_RhYxL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация фраз (процесс разбиения фраз на отдельные слова)"
      ],
      "metadata": {
        "id": "PPbc9e4ZhbGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sent(sent):\n",
        "    words = word_tokenize(sent, language='russian')\n",
        "    return words"
      ],
      "metadata": {
        "id": "Rn5SMxlXhfo5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sents(sents):\n",
        "    words = []\n",
        "    for s in sents:\n",
        "        w = tokenize_sent(s)\n",
        "        words = words + w\n",
        "    return words"
      ],
      "metadata": {
        "id": "KZtqPx3ohjvq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаление ссылок"
      ],
      "metadata": {
        "id": "4e6VzRLIiD5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_urls(text):\n",
        "    text = re.sub(r\"https?://[^,\\s]+,?\", \"\", text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "jMVHoRO2h10K"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стемминг, лемматизация, стоп-слова"
      ],
      "metadata": {
        "id": "mCN-OLAGiLNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# таблица пакетной замены символов\n",
        "PUNCTUATION_STRING = '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~«»'#-\n",
        "punct_str = str.maketrans('', '', PUNCTUATION_STRING)\n",
        "\n",
        "# алгоритм, используемый для лемматизации (приведения к нормальной форме) слов\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# алгоритм стемминга (нахождения основы) слов\n",
        "stemmer = SnowballStemmer(language=\"russian\")\n",
        "\n",
        "# стоп-слова\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "\n",
        "stop_words.extend(['который', 'которая', 'также', 'такой', 'однако', 'это'])"
      ],
      "metadata": {
        "id": "di3v9-n2isrE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стемминг слов"
      ],
      "metadata": {
        "id": "U9DONUA1ivWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(word):\n",
        "    word = stemmer.stem(word)\n",
        "    return word"
      ],
      "metadata": {
        "id": "75erPtn3iz4S"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемматизация слов"
      ],
      "metadata": {
        "id": "bua6HwT1i2pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(word):\n",
        "    word = morph.parse(word)[0].normal_form\n",
        "    return word"
      ],
      "metadata": {
        "id": "AZv1x6sxi4by"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удаление цифр и знаков пунктуации"
      ],
      "metadata": {
        "id": "WNHtck8zi8Ma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(word):\n",
        "    word = ''.join([x for x in word if not x.isdigit()])\n",
        "    word = ''.join([x.translate(punct_str) for x in word])\n",
        "    return word"
      ],
      "metadata": {
        "id": "4S2kRGEti92K"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Визуализация с помощью \"облака слов\""
      ],
      "metadata": {
        "id": "cGeFuYUBjBcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus(data):\n",
        "    corpus = []\n",
        "    for phrase in data:\n",
        "        for word in phrase.split():\n",
        "            corpus.append(word)\n",
        "    return corpus\n",
        "\n",
        "def str_corpus(corpus):\n",
        "    str_corpus = ' '.join(corpus)\n",
        "    return str_corpus\n",
        "\n",
        "def get_wordCloud(corpus):\n",
        "    word_сloud = WordCloud(background_color='white', width=800, height=600, max_words=100).generate(str_corpus(corpus))\n",
        "    return word_сloud\n",
        "\n",
        "def get_top_words(text):\n",
        "    word_cloud = get_wordCloud(text)\n",
        "    return word_cloud.words_.keys()\n",
        "\n",
        "def show_wordCloud(text):\n",
        "    word_cloud = get_wordCloud(text)\n",
        "    fig = plt.figure(figsize=(30, 10))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(word_cloud)"
      ],
      "metadata": {
        "id": "DVker96SjDZy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция предобработки текста (на входе - текст, на выходе - список обработанных слов):\n",
        "\n",
        "* приведение к нижнему регистру\n",
        "* удаление чисел и знаков пунктуации\n",
        "* исключение стоп-слов\n",
        "* лемматизация / стемминг слов\n",
        "* исключение дополнительных стоп-слов"
      ],
      "metadata": {
        "id": "JmM7zlLojL1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_words_preprocessing(\n",
        "        text, # текстовая строка\n",
        "        transform=True, # флаг обработки слов\n",
        "        mode='lemma', # тип нормализации слов: 'lemma' (по умолчанию), 'stem'\n",
        "        add_stop_words=[] # список дополнительных стоп-слов\n",
        "    ):\n",
        "    text = text.replace('-', ' ')\n",
        "    sents = tokenize_text(text)\n",
        "    words = tokenize_sents(sents)\n",
        "    words = [x.lower() for x in words]\n",
        "    words = [clean(x) for x in words]\n",
        "    words = [x for x in words if len(x) > 2]\n",
        "    if transform:\n",
        "        words = [x for x in words if x not in stop_words]\n",
        "        if mode == 'lemma':\n",
        "            words = [lemmatize(x) for x in words]\n",
        "        elif mode == 'stem':\n",
        "            words = [stemming(x) for x in words]\n",
        "        if len(add_stop_words):\n",
        "            words = [x for x in words if x not in add_stop_words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "E9hw6rxjjI-i"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция подготовки датасета"
      ],
      "metadata": {
        "id": "vAhckraRjSD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocessing(\n",
        "        df,\n",
        "        transform=True, # флаг обработки слов\n",
        "        aug=False, # флаг аугментации текста\n",
        "        mode='lemma', # тип нормализации слов: lemma (по умолчанию), stem\n",
        "        local_stop_words=[] # список дополнительных стоп-слов\n",
        "    ):\n",
        "\n",
        "    # дополнение, лемматизация/стемминг стоп-слов\n",
        "    stop_words_set = stop_words.copy()\n",
        "    stop_words_set.extend(local_stop_words)\n",
        "    if mode == 'lemma':\n",
        "        stop_words_set = set([lemmatize(x) for x in stop_words_set])\n",
        "    elif mode == 'stem':\n",
        "        stop_words_set = set([stemming(x) for x in stop_words_set])\n",
        "\n",
        "    # предобработка текста\n",
        "    df['text'] = [\n",
        "        ' '.join(text_to_words_preprocessing(\n",
        "            x,\n",
        "            mode=mode,\n",
        "            transform=transform,\n",
        "            add_stop_words=stop_words_set\n",
        "            )\n",
        "        ) for x in tqdm(df['content'], leave=False)\n",
        "    ]\n",
        "\n",
        "    # удаляем пустые значения\n",
        "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "    df = remove_empty(df)\n",
        "\n",
        "    # удаляем элементы, где недостаточно текста\n",
        "    df[df['text'].str.len() >= MIN_TEXT_LEN]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "diGzzrt6jU4C"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовим загруженные данные"
      ],
      "metadata": {
        "id": "ZajiV2Hpjt0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = remove_empty(df) # удаляем пустые значения\n",
        "df = set_category_type(df) # присваиваем типы колонкам"
      ],
      "metadata": {
        "id": "Xj0Pj1tBjsfz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проанализируем состав текста до обработки"
      ],
      "metadata": {
        "id": "ETQNX_Cwj5c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_wordCloud(df['content'])"
      ],
      "metadata": {
        "id": "f1eaPyJc9Wa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перечислим стоп-слова, специфические для данного датасета"
      ],
      "metadata": {
        "id": "dQPKSeKlmo3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "local_stop_words = [\n",
        "    'январь', 'февраль', 'март', 'апрель', 'май', 'июнь', 'июль', 'август', 'сентябрь', 'октябрь', 'ноябрь', 'декабрь',\n",
        "    'новости', 'агентство', 'фото', 'фотохост', 'фотохостагентство', 'москва', 'риа', 'фонтанка', 'пресс', 'прессслужба',\n",
        "    'поделиться', 'перейти', 'istockcom', 'медиабанк'\n",
        "]"
      ],
      "metadata": {
        "id": "X1EXJLtJmhe7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Формирование датасета"
      ],
      "metadata": {
        "id": "0MBDQzEwkFqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_preprocessing(df, mode='lemma', transform=True, local_stop_words=local_stop_words)\n",
        "data = data.drop(columns=['content'])\n",
        "\n",
        "dataset_path = DATA_DIR + DATASET_DIR + 'news_data.csv'\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "data.to_csv(dataset_path, sep=';')\n",
        "\n",
        "print('Dataset file:', dataset_path)\n",
        "print('Total news:', len(data))"
      ],
      "metadata": {
        "id": "aonDVXhS9ZK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проанализируем состав текста после обработки"
      ],
      "metadata": {
        "id": "6AoFIA3OkUEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_wordCloud(data['text'])"
      ],
      "metadata": {
        "id": "DDVIOvNH9yiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формируем и сохраняем тестовый датасет"
      ],
      "metadata": {
        "id": "i4CifxCPkYH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = DATA_DIR + TEST_DIR + 'test_news.csv'\n",
        "\n",
        "df_test = pd.read_csv(test_path, usecols=['content'])\n",
        "df_test = df_test[:200].astype({'content': str})\n",
        "df_test = remove_empty(df_test)\n",
        "\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "zqX2DRIn90sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = data_preprocessing(df_test, mode='lemma', local_stop_words=local_stop_words)\n",
        "df_test = pd.DataFrame(df_test)\n",
        "df_test = df_test.drop(columns=['content'])\n",
        "df_test = df_test.replace(r'^\\s*$', np.nan, regex=True)\n",
        "df_test = remove_empty(df_test)"
      ],
      "metadata": {
        "id": "ZrjHD52o932r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_path = DATA_DIR + DATASET_DIR + 'test_data.csv'\n",
        "\n",
        "df_test.to_csv(dataset_test_path, sep=',')\n",
        "\n",
        "print('Test dataset file:', dataset_test_path)\n",
        "print('Total news:', len(df_test))"
      ],
      "metadata": {
        "id": "Q3fYjbzp-HCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение модели"
      ],
      "metadata": {
        "id": "ehTfrpukvUS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция присвоения строкового типа колонке `text`, категориального типа колонке `category`"
      ],
      "metadata": {
        "id": "gKYSfxD2vY4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_category_type(df):\n",
        "    df = df.astype({'text': str, 'category': 'category'})\n",
        "    return df"
      ],
      "metadata": {
        "id": "_UWhHFD7vbXW"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Читаем датасет для обучения"
      ],
      "metadata": {
        "id": "G07Z7RI6vg4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = DATA_DIR + DATASET_DIR + 'news_data.csv'\n",
        "dataset = pd.read_csv(dataset_path, delimiter=';', usecols=['text', 'category'])"
      ],
      "metadata": {
        "id": "LRYQonKHvlqT"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Список категорий"
      ],
      "metadata": {
        "id": "CMcZGLHLwElN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_set = dataset['category'].unique()\n",
        "category_set.sort()"
      ],
      "metadata": {
        "id": "PE1zQllMwAyW"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Формируем тренировочный датасет"
      ],
      "metadata": {
        "id": "B2UGaN3bwH0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = set_category_type(dataset)"
      ],
      "metadata": {
        "id": "NoGwbACWwGhT"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Настройки балансировки датасета"
      ],
      "metadata": {
        "id": "3-ro93ouweie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_len = 350000 # общий размер датасета\n",
        "balance_weigths = [0.48, 0.11, 0.07, 0.12, 0.06, 0.03, 0.04, 0.04, 0.05] # веса категорий\n",
        "\n",
        "print(sum(balance_weigths))"
      ],
      "metadata": {
        "id": "BNi_CsXR-J3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balance_set = [round(x * total_len) for x in balance_weigths]\n",
        "balance_size = dict(enumerate(balance_set)) # размеры датасета по пубрикам\n",
        "\n",
        "print(balance_size)"
      ],
      "metadata": {
        "id": "JSGBUUXb_Qht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для балансировки датасета используем класс RandomOverSampler из библиотеки imbalanced-learn"
      ],
      "metadata": {
        "id": "vxlwS9A5wTVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp = pd.DataFrame()\n",
        "for i in range(len(balance_size)):\n",
        "  x = df[df['category'] == i][:balance_set[i]]\n",
        "  a = pd.DataFrame(data=x)\n",
        "  df_tmp = pd.concat([df_tmp, a], ignore_index=True)"
      ],
      "metadata": {
        "id": "vjGfGD8dw7n-"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "over_sampler = RandomOverSampler(sampling_strategy=balance_size, random_state=RANDOM_STATE)\n",
        "df, _ = over_sampler.fit_resample(df_tmp, df_tmp['category'])"
      ],
      "metadata": {
        "id": "D_J4M_CAxzoc"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Распределение записей по категориям после балансировки"
      ],
      "metadata": {
        "id": "JixvUe4nywYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(15, 5)})\n",
        "sns.countplot(x='category', data=df)\n",
        "plt.xlabel(\"category\", size = 12)\n",
        "plt.ylabel(\"count\", size = 12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j0N8oYXe-Pw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перемешиваем датасет"
      ],
      "metadata": {
        "id": "xaTr0yvLy7J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1)"
      ],
      "metadata": {
        "id": "tZp6azyYy-4s"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим данные на тренировочную и тестовую выборки"
      ],
      "metadata": {
        "id": "rgZdovsoy-ZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df, df['category'], train_size=0.8, random_state=RANDOM_STATE, stratify=df['category'])\n",
        "\n",
        "X_train = X_train['text']\n",
        "X_test = X_test['text']"
      ],
      "metadata": {
        "id": "yIK9iRqyzE3v"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция вывода матрицы соответствий"
      ],
      "metadata": {
        "id": "ktMd9pXbzPtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cm_plot(y_test, y_pred, labels=None):\n",
        "    sns.set(font_scale=1.3)\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
        "    cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(cmn, fmt='.2f', cmap='Blues', annot=True, cbar=False, xticklabels=labels, yticklabels=labels)\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    return plt.show()"
      ],
      "metadata": {
        "id": "8C8xfwwwzRTH"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберем векторизатор и классификатор"
      ],
      "metadata": {
        "id": "pdE3MFTPzZno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectorizer(v, params):\n",
        "    if v == 'tfidf':\n",
        "        vectorizer = TfidfVectorizer(**params)\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "    return vectorizer"
      ],
      "metadata": {
        "id": "vSGXsvbuzh2l"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classifier(c):\n",
        "    if c == 'LGR':\n",
        "        clf = LogisticRegression(random_state=RANDOM_STATE)\n",
        "        parameters = {}\n",
        "    elif c == 'SVC':\n",
        "        clf = LinearSVC(random_state=RANDOM_STATE, max_iter=10000, multi_class='crammer_singer')\n",
        "        parameters = {}\n",
        "    elif c == 'SGD':\n",
        "        clf = SGDClassifier(random_state=RANDOM_STATE, n_iter_no_change=5)\n",
        "        parameters = {}\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "    return clf, parameters"
      ],
      "metadata": {
        "id": "HTeH-uX4zlNu"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция проведения экспериментов"
      ],
      "metadata": {
        "id": "I1G3zPEh1Cnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def do_experiments():\n",
        "    model_names = []\n",
        "    model_best_params = {}\n",
        "    model_estim = {}\n",
        "    model_best_metric = {}\n",
        "\n",
        "    for c in classifiers:\n",
        "        model_estim[c] = []\n",
        "        model_best_metric[c] = 0\n",
        "\n",
        "    for v in vectorizers:\n",
        "        params_set = [\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.1, 'min_df': 1},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.1, 'min_df': 3},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.1, 'min_df': 5},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.5, 'min_df': 1},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.5, 'min_df': 3},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.5, 'min_df': 5},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.9, 'min_df': 1},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.9, 'min_df': 3},\n",
        "            {'ngram_range': (1, 2), 'max_df': 0.9, 'min_df': 5}\n",
        "        ]\n",
        "\n",
        "        for vect_params in params_set:\n",
        "            vectorizer = get_vectorizer(v, vect_params)\n",
        "            vector_train = vectorizer.fit_transform(X_train)\n",
        "\n",
        "            model_name = vect_params\n",
        "            model_names.append(model_name)\n",
        "\n",
        "            for c in classifiers:\n",
        "\n",
        "                clf, clf_parameters = get_classifier(c)\n",
        "                clf.fit(vector_train, y_train)\n",
        "\n",
        "                vector_test = vectorizer.transform(X_test)\n",
        "\n",
        "                prediction = clf.predict(vector_test)\n",
        "                model_metric = accuracy_score(prediction, y_test)\n",
        "\n",
        "                print(c, model_metric, vect_params)\n",
        "\n",
        "                model_estim[c].append(model_metric)\n",
        "\n",
        "                if model_metric > model_best_metric[c]:\n",
        "                    model_best_metric[c] = model_metric\n",
        "                    model_best_params[c] = [vect_params]\n",
        "\n",
        "    return model_names, model_estim, model_best_params"
      ],
      "metadata": {
        "id": "h4pUFuZ20qSl"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Проведение экспериментов"
      ],
      "metadata": {
        "id": "Y5S_KRJt1ehZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizers = ['tfidf']\n",
        "classifiers = ['SVC', 'SGD']"
      ],
      "metadata": {
        "id": "zsExwrfrzX5m"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_list, estimation, best_params = do_experiments()"
      ],
      "metadata": {
        "id": "d7IeBmc6-TaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = estimation.keys()\n",
        "clfs_score = {}\n",
        "width = 1 / (len(clfs) + 1)\n",
        "x_shift = (len(clfs) - 1) / 2\n",
        "y_min = 1\n",
        "y_max = 0\n",
        "\n",
        "x = np.arange(len(datasets_list))\n",
        "sns.set_theme()\n",
        "sns.set(rc={'figure.figsize':(15,4)})\n",
        "\n",
        "colors = ['#ff6b81', '#1e90ff', '#2ed573']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "i = 0\n",
        "for clf_id in clfs:\n",
        "  ax.bar(x + (i - x_shift) * width, estimation[clf_id], width, label=clf_id, color=colors[i])\n",
        "  i += 1\n",
        "  y_min = min(y_min, min(estimation[clf_id]))\n",
        "  y_max = max(y_max, max(estimation[clf_id]))\n",
        "  clfs_score[clf_id] = max(estimation[clf_id])\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(datasets_list)\n",
        "ax.set_ylim(y_min * 0.99, y_max * 1.005)\n",
        "ax.legend()\n",
        "plt.title('Метрика Accuracy')"
      ],
      "metadata": {
        "id": "8GBiO0fo-WJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберем лучший классификатор"
      ],
      "metadata": {
        "id": "WZvoA5CD11Ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_clf = max(clfs_score, key=clfs_score.get)\n",
        "print('Best classifier:', best_clf)"
      ],
      "metadata": {
        "id": "de-KrFOG-YQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним векторизацию с использованием лучших параметров"
      ],
      "metadata": {
        "id": "9ZlLhEZ62Nqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_max = best_params[best_clf][0]\n",
        "print('Best parameters:', best_params_max)"
      ],
      "metadata": {
        "id": "vg0vzH47-a9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = get_vectorizer('tfidf', best_params_max)\n",
        "vector_train = vectorizer.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "Co7Rn8FQ0LPn"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним обучение и тестирование"
      ],
      "metadata": {
        "id": "mEhJY_jp2ZXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf, clf_parameters = get_classifier(best_clf)\n",
        "clf.fit(vector_train, y_train)"
      ],
      "metadata": {
        "id": "IANUtDnz-dfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним предсказание на тестовых данных"
      ],
      "metadata": {
        "id": "o35sc7hy2fhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_test = vectorizer.transform(X_test)\n",
        "y_pred = clf.predict(vector_test)"
      ],
      "metadata": {
        "id": "31kA0gDP2Z_p"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_metric = accuracy_score(y_pred, y_test)\n",
        "print('Accuracy:', model_metric)"
      ],
      "metadata": {
        "id": "3W2WiD_r-fgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем матрицу соответствий"
      ],
      "metadata": {
        "id": "CmY3LZSk2yGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm_plot(y_test, y_pred, labels=category_set)"
      ],
      "metadata": {
        "id": "fBVhPFrA-g9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестовое предсказание"
      ],
      "metadata": {
        "id": "kIMhfgFR6TXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модель на всех данных"
      ],
      "metadata": {
        "id": "JMBRznU76QCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all = df['text']\n",
        "y_train_all = df['category']"
      ],
      "metadata": {
        "id": "Jn0w9sbm6Pb3"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_train_all = vectorizer.fit_transform(X_train_all)"
      ],
      "metadata": {
        "id": "CvtrQFQd6bhp"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(vector_train_all, y_train_all)"
      ],
      "metadata": {
        "id": "jc_0ppsn-jEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news_test = pd.read_csv(dataset_test_path, sep=';')\n",
        "\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "iAxENwWi-kuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_news_test = vectorizer.transform(df_test['text'])\n",
        "y_pred_news_test = clf.predict(vector_news_test)\n",
        "df_test['topic'] = y_pred_news_test"
      ],
      "metadata": {
        "id": "SfDBvPgJ67c4"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраняем данные для отправки на конкурс\n"
      ],
      "metadata": {
        "id": "us9vDO7i7RMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit_path = DATA_DIR + SUBMIT_DIR + \"submission.csv\"\n",
        "\n",
        "df_test['topic'].to_csv(submit_path, sep=',', index_label='index')\n",
        "\n",
        "print('Submission file:', submit_path)"
      ],
      "metadata": {
        "id": "18wznKVL-wsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}